{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to my_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"dataset/\"\n",
    "\n",
    "# List of authors\n",
    "authors = [\"alen markaryan\", \"cevdet erdöl\", \"engin verel\", \"serkan fıçıcı\", \"taceddin kutay\"]\n",
    "\n",
    "# Dictionary to store text data for each author\n",
    "data_dict = {\"Author\": [], \"Text\": []}\n",
    "\n",
    "# Loop through each author\n",
    "for author in authors:\n",
    "    author_folder = os.path.join(data_directory, author)\n",
    "\n",
    "    # Loop through each text file in the author's folder\n",
    "    for filename in os.listdir(author_folder):\n",
    "        file_path = os.path.join(author_folder, filename)\n",
    "\n",
    "        # Read the content of the text file with error handling\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Append data to the dictionary\n",
    "        data_dict[\"Author\"].append(author)\n",
    "        data_dict[\"Text\"].append(text)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = \"my_dataset.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"my_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Author                                               Text\n",
      "0  alen markaryan  Son oynadığı deplasman maçında 6 yiyen takımla...\n",
      "1  alen markaryan  \\nSon iki aydır Beşiktaş camiasında yoğunlaşan...\n",
      "2  alen markaryan  \\n\\nBeşiktaş maçı varsa Beşiktaş maçı konuşulu...\n",
      "3  alen markaryan  \\n\\nPazar günü F.Bahçe sahasında K.Gümrük'le o...\n",
      "4  alen markaryan  \\n\\nAlmanya'da Almanya ile Türk vatandaşların ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105 entries, 0 to 104\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Author  105 non-null    object\n",
      " 1   Text    105 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.8+ KB\n",
      "None\n",
      "                Author                                               Text\n",
      "count              105                                                105\n",
      "unique               5                                                104\n",
      "top     alen markaryan  Haberler...\\n\\nYorumlar...\\n\\nAnalizler...\\n\\n...\n",
      "freq                21                                                  2\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    #to lowercase\n",
    "    lowercase_text = text.lower()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    clean_html = re.sub(r'<.*?>', '', lowercase_text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    clean_whitespace = re.sub(r'\\s+', ' ', clean_html).strip()\n",
    "    \n",
    "    return clean_whitespace\n",
    "\n",
    "def clean_csv(input_csv, output_csv):\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Clean the 'text' column\n",
    "    df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "    df = df.drop('Text', axis=1)\n",
    "\n",
    "    # Save cleaned data to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "input_csv_file = 'my_dataset.csv' \n",
    "output_csv_file = 'cleaned1_output_file.csv' \n",
    "\n",
    "clean_csv(input_csv_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization:\n",
    "import pandas as pd\n",
    "from zemberek import TurkishTokenizer\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenizer = TurkishTokenizer.DEFAULT\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_contents = [token.content for token in tokens]\n",
    "    return token_contents\n",
    "\n",
    "def tokenize_csv(input_csv, output_csv):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Tokenize the 'cleaned_text' column\n",
    "    df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "    df = df.drop('cleaned_text', axis=1)\n",
    "\n",
    "    # Save tokenized data to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "input_csv_file = 'cleaned1_output_file.csv'  \n",
    "output_csv_file = 'tokenized_output_file.csv' \n",
    "\n",
    "tokenize_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  \n",
    "import string\n",
    "from zemberek import TurkishTokenizer\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    # Define a translation table to remove punctuation\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Remove punctuation from each token\n",
    "    tokens_without_punctuation = [token.translate(translator) for token in tokens]\n",
    "\n",
    "    # Remove empty tokens resulting from the translation\n",
    "    tokens_without_punctuation = [token for token in tokens_without_punctuation if token]\n",
    "\n",
    "    return tokens_without_punctuation\n",
    "\n",
    "def process_csv(input_csv, output_csv):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Convert the 'tokens' column from string to list\n",
    "    df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "    # Remove punctuation from the 'tokens' column\n",
    "    df['tokens'] = df['tokens'].apply(remove_punctuation)\n",
    "\n",
    "    # Save processed data to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "input_csv_file = 'tokenized_output_file.csv'  \n",
    "output_csv_file = 'processed_output_file.csv'  \n",
    "\n",
    "process_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "from zemberek import TurkishTokenizer\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    # Remove stop words from the tokens\n",
    "    tokens_without_stopwords = [token for token in tokens if token.lower() not in stopwords]\n",
    "\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "def process_csv(input_csv, output_csv, stopwords_file):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Convert the 'tokens' column from string to list\n",
    "    df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "    # Read stop words from the TXT file\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as stopword_file:\n",
    "        stop_words = set(stopword_file.read().splitlines())\n",
    "\n",
    "    # Remove stop words from the 'tokens' column\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: remove_stopwords(x, stop_words))\n",
    "\n",
    "    # Save processed data to a new CSV file\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')  \n",
    "\n",
    "\n",
    "input_csv_file = 'processed_output_file.csv'  \n",
    "output_csv_file = 'nostopwords_output_file0.csv'  \n",
    "stopwords_file = 'stopwords.txt'  \n",
    "\n",
    "process_csv(input_csv_file, output_csv_file, stopwords_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zemberek import TurkishMorphology\n",
    "import ast\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    morphology = TurkishMorphology.create_with_defaults()\n",
    "    all_lemmas = []\n",
    "\n",
    "    for token in tokens:\n",
    "        analysis = morphology.analyze(token)\n",
    "        lemmas = [result.get_stem()  for result in analysis.analysis_results]\n",
    "        if lemmas:\n",
    "            lemmas = lemmas[0]    \n",
    "            all_lemmas.append(lemmas)\n",
    "                \n",
    "    \n",
    "    return all_lemmas\n",
    "\n",
    "\n",
    "def process_csv(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Convert the 'tokens' column from string to list\n",
    "    df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "    # Apply lemmatization to the 'tokens' column\n",
    "    df['tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "    # Save processed data to a new CSV file\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "input_csv_file = 'nostopwords_output_file0.csv'\n",
    "output_csv_file = 'lemmatized_output_file.csv'\n",
    "process_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     01  02  10  100  10000  100000  1015  103  11  1187  ...  şuur  şöyle  \\\n",
      "0     1   0   0    0      0       0     0    0   1     0  ...     0      0   \n",
      "1     0   0   3    0      0       0     0    0   0     0  ...     0      0   \n",
      "2     0   0   0    0      0       0     0    0   1     0  ...     0      0   \n",
      "3     0   0   0    0      0       0     0    0   1     0  ...     0      0   \n",
      "4     0   0   0    0      0       0     0    0   0     0  ...     0      0   \n",
      "..   ..  ..  ..  ...    ...     ...   ...  ...  ..   ...  ...   ...    ...   \n",
      "100   0   0   0    0      0       0     0    0   0     0  ...     0      0   \n",
      "101   0   0   0    0      0       0     0    0   0     0  ...     0      0   \n",
      "102   0   0   0    0      0       0     0    0   0     0  ...     0      0   \n",
      "103   0   0   0    0      0       0     0    0   0     0  ...     0      1   \n",
      "104   0   0   0    0      0       0     0    0   0     0  ...     0      0   \n",
      "\n",
      "     şükran  şükred  şükret  şükür  şüphe  şık  şıkır  şıra  \n",
      "0         0       0       0      0      0    0      0     0  \n",
      "1         0       0       0      0      0    0      0     0  \n",
      "2         0       0       0      0      0    0      0     0  \n",
      "3         0       0       0      0      0    0      0     0  \n",
      "4         0       0       0      0      0    0      2     0  \n",
      "..      ...     ...     ...    ...    ...  ...    ...   ...  \n",
      "100       0       0       0      1      0    0      0     0  \n",
      "101       0       0       0      0      0    0      0     0  \n",
      "102       0       0       0      0      0    0      0     0  \n",
      "103       0       0       0      0      0    0      0     0  \n",
      "104       0       0       0      0      0    0      0     0  \n",
      "\n",
      "[105 rows x 4908 columns]\n"
     ]
    }
   ],
   "source": [
    "#  Applying Bag of Words on data:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "df = pd.read_csv('lemmatized_output_file.csv', encoding='utf-8')  \n",
    "\n",
    "\n",
    "\n",
    "corpus = df['tokens']\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into a sparse matrix\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(df_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9523809523809523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "y = df['Author']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=10000)  \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "joblib.dump(model, 'trained_model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('trained_model.pkl')\n",
    "\n",
    "# Load the vectorizer\n",
    "vectorizer = joblib.load('vectorizer.pkl')\n",
    "\n",
    "# Preprocess the real text\n",
    "with open('testing_a_text.txt', 'r', encoding='utf-8') as file:\n",
    "    real_text = file.read()\n",
    "\n",
    "\n",
    "# Apply the vectorizer to the preprocessed real text\n",
    "real_text_bow = vectorizer.transform([real_text])\n",
    "\n",
    "# Make predictions on the real text\n",
    "real_text_predictions = model.predict(real_text_bow)\n",
    "\n",
    "\n",
    "print(\"Predicted Author:\", real_text_predictions[0])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
